inferenceService:
  name: cpu-inference-service
  minReplicas: 1
  maxReplicas: 1
  resources:
    requests:
      cpu: "4"
      memory: 8Gi
    limits:
      cpu: "8"
      memory: 16Gi
  affinity: {}
  tolerations: {}

servingRuntime:
  name: cpu-runtime
  port: 8080
  image: ghcr.io/ggml-org/llama.cpp:server
  modelFormat: llama.cpp
  args:
    - --model
    - /models/mistral-7b-instruct-v0.2.Q5_0.gguf

model:
  repository: TheBloke/Mistral-7B-Instruct-v0.2-GGUF
  filename: mistral-7b-instruct-v0.2.Q5_0.gguf # An empty string downloads the entire repo
  storage:
    mountPath: /models
  downloader:
    image: registry.access.redhat.com/ubi10/python-312-minimal:10.0

dsc:
  initialize: true
  kserve:
    defaultDeploymentMode: RawDeployment
    rawDeploymentServiceConfig: Headed

externalSecret:
  enabled: true
